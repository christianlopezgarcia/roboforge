{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ec1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\n",
      "Requirement already satisfied: pip in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (25.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\n",
      "Requirement already satisfied: opencv-python-headless in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (2.2.6)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu, https://www.piwheels.org/simple\n",
      "Requirement already satisfied: torch in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (2.8.0+cpu)\n",
      "Requirement already satisfied: torchvision in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from jinja2->torch) (3.0.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\n",
      "Requirement already satisfied: ultralytics in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (8.3.207)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (1.16.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (2.8.0+cpu)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (1.34.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: filelock in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: polars-runtime-32==1.34.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from polars->ultralytics) (1.34.0)\n"
     ]
    }
   ],
   "source": [
    "# Make sure pip is up to date\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install OpenCV\n",
    "!pip3 install opencv-python-headless numpy\n",
    "\n",
    "# Install PyTorch (CPU-only for Pi 4B)\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Install ultralytics (YOLO v8+)\n",
    "!pip3 install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9337a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you haven't installed yet\n",
    "# !pip install --upgrade pip\n",
    "# !pip install opencv-python-headless numpy ultralytics torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Dependencies imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1299b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path to your trained model\n",
    "MODEL_PATH = \"trained_yolo_model/ODM-ver5.pt\"\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(\"YOLO model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7acd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration: place a single block exactly 12 inches away and press SPACE\n",
      "\n",
      "0: 480x640 (no detections), 3243.2ms\n",
      "Speed: 27.8ms preprocess, 3243.2ms inference, 9.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m results = model(frame)\n\u001b[32m     16\u001b[39m annotated = results[\u001b[32m0\u001b[39m].plot()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCalibration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m key = cv2.waitKey(\u001b[32m1\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key & \u001b[32m0xFF\u001b[39m == \u001b[32m32\u001b[39m:  \u001b[38;5;66;03m# SPACE key\u001b[39;00m\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# Calibration parameters\n",
    "KNOWN_WIDTH_INCH = 1.0      # block width\n",
    "KNOWN_DISTANCE_INCH = 12.0  # place block at this distance for calibration\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "FOCAL_LENGTH = None\n",
    "\n",
    "print(\"Calibration: place a single block exactly 12 inches away and press SPACE\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    results = model(frame)\n",
    "    annotated = results[0].plot()\n",
    "    \n",
    "    cv2.imshow(\"Calibration\", annotated)\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    if key & 0xFF == 32:  # SPACE key\n",
    "        boxes = results[0].boxes.xywh  # x_center, y_center, w, h\n",
    "        if len(boxes) > 0:\n",
    "            w_pixels = boxes[0][2].item()\n",
    "            FOCAL_LENGTH = (w_pixels * KNOWN_DISTANCE_INCH) / KNOWN_WIDTH_INCH\n",
    "            print(f\"✅ Focal length calibrated: {FOCAL_LENGTH:.2f} pixels\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"⚠️ No block detected. Try again\")\n",
    "    elif key & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f7fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "KNOWN_WIDTH_INCH = 1.0\n",
    "KNOWN_DISTANCE_INCH = 12.0\n",
    "FOCAL_LENGTH = None\n",
    "\n",
    "# Load your model if not already loaded\n",
    "# model = YOLO(MODEL_PATH)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Camera not opened\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae788e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\n",
      "Requirement already satisfied: opencv-python-headless in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (3.10.7)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from opencv-python-headless) (2.2.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/clopezgarcia2/Desktop/robo_env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python-headless matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf014b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'picamera2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpicamera2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Picamera2\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'picamera2'"
     ]
    }
   ],
   "source": [
    "from picamera2 import Picamera2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "picam2 = Picamera2()\n",
    "picam2.start()\n",
    "\n",
    "while True:\n",
    "    frame = picam2.capture_array()  # returns a NumPy array\n",
    "    # frame is now an OpenCV BGR image\n",
    "    cv2.imshow(\"Preview\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "picam2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf61ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[119, 146, 172],\n",
       "        [121, 148, 174],\n",
       "        [122, 152, 177],\n",
       "        ...,\n",
       "        [161, 171, 188],\n",
       "        [160, 170, 187],\n",
       "        [160, 170, 187]],\n",
       "\n",
       "       [[120, 147, 173],\n",
       "        [122, 149, 175],\n",
       "        [123, 153, 178],\n",
       "        ...,\n",
       "        [161, 171, 188],\n",
       "        [160, 170, 187],\n",
       "        [160, 170, 187]],\n",
       "\n",
       "       [[123, 150, 176],\n",
       "        [124, 151, 177],\n",
       "        [125, 155, 180],\n",
       "        ...,\n",
       "        [161, 171, 188],\n",
       "        [160, 170, 187],\n",
       "        [160, 170, 187]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[183, 182, 186],\n",
       "        [179, 178, 182],\n",
       "        [180, 179, 183],\n",
       "        ...,\n",
       "        [121, 111, 117],\n",
       "        [113, 103, 109],\n",
       "        [115, 105, 111]],\n",
       "\n",
       "       [[165, 164, 168],\n",
       "        [173, 172, 176],\n",
       "        [187, 186, 190],\n",
       "        ...,\n",
       "        [102,  92,  98],\n",
       "        [101,  91,  97],\n",
       "        [103,  93,  99]],\n",
       "\n",
       "       [[123, 122, 126],\n",
       "        [145, 144, 148],\n",
       "        [176, 175, 179],\n",
       "        ...,\n",
       "        [ 95,  85,  91],\n",
       "        [ 96,  86,  92],\n",
       "        [ 98,  88,  94]]], shape=(1080, 810, 3), dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "KNOWN_WIDTH_INCH = 1.0\n",
    "KNOWN_DISTANCE_INCH = 12.0\n",
    "FOCAL_LENGTH = None\n",
    "\n",
    "# Load your model if not already loaded\n",
    "# model = YOLO(MODEL_PATH)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Calibration: place a single block 12 inches away and press SPACE\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    start = time.time()\n",
    "    results = model(frame, verbose=False)  # suppress extra logs\n",
    "    end = time.time()\n",
    "    \n",
    "    # Annotate frame\n",
    "    annotated_frame = results[0].plot()\n",
    "    fps = 1 / (end - start)\n",
    "    cv2.putText(annotated_frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Convert BGR to RGB for Jupyter\n",
    "    annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display in Jupyter\n",
    "    clear_output(wait=True)\n",
    "    display(cv2.cvtColor(annotated_frame_rgb, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Check if any block detected\n",
    "    if len(results[0].boxes) > 0:\n",
    "        print(f\"Detected {len(results[0].boxes)} block(s)\")\n",
    "\n",
    "    key = input(\"Press 'c' to calibrate, 'q' to quit, Enter to continue... \").lower()\n",
    "    if key == 'c':\n",
    "        if len(results[0].boxes) > 0:\n",
    "            w_pixels = results[0].boxes.xywh[0][2].item()\n",
    "            FOCAL_LENGTH = (w_pixels * KNOWN_DISTANCE_INCH) / KNOWN_WIDTH_INCH\n",
    "            print(f\"✅ Focal length calibrated: {FOCAL_LENGTH:.2f} pixels\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"⚠️ No block detected, try again\")\n",
    "    elif key == 'q':\n",
    "        break\n",
    "\n",
    "cap.release() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83996f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83739e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@200.049] global cap_v4l.cpp:914 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@200.069] global obsensor_uvc_stream_channel.cpp:163 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting live distance estimation. Press ESC to quit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting live distance estimation. Press ESC to quit.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if FOCAL_LENGTH is not None:\n",
    "    print(\"⚠️ Focal length not calibrated. Run calibration first.\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Starting live distance estimation. Press ESC to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        results = model(frame)\n",
    "        annotated = results[0].plot()\n",
    "\n",
    "        for box in results[0].boxes.xywh:\n",
    "            w_pixels = box[2].item()\n",
    "            distance_inch = (KNOWN_WIDTH_INCH * FOCAL_LENGTH) / w_pixels\n",
    "            x, y = int(box[0]), int(box[1])\n",
    "            cv2.putText(annotated, f\"{distance_inch:.2f} in\", (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"YOLO Distance Estimation\", annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f2fc29",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert BGR -> RGB for Jupyter display\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m frame_rgb = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m display(frame_rgb)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.12.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "\n",
    "# Convert BGR -> RGB for Jupyter display\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "clear_output(wait=True)\n",
    "display(frame_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5665ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# --- Parameters ---\n",
    "MODEL_PATH = \"trained_yolo_model/ODM-ver5.pt\"\n",
    "KNOWN_WIDTH_INCH = 1.0        # Width of block\n",
    "ASSUMED_PIXEL_WIDTH = 100.0   # Approx width of block in pixels at ~12 inches\n",
    "FOCAL_LENGTH = (ASSUMED_PIXEL_WIDTH * 12.0) / KNOWN_WIDTH_INCH  # pixels\n",
    "\n",
    "# --- Load YOLO model ---\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(\"✅ YOLO model loaded\")\n",
    "\n",
    "# --- Open camera ---\n",
    "cap = cv2.VideoCapture(\"/dev/video0\")  # change to /dev/video1 if needed\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "print(\"Starting live YOLO detection. Press ESC to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"❌ Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = model(frame, verbose=False)  # run YOLO inference\n",
    "    end_time = time.time()\n",
    "\n",
    "    annotated = results[0].plot()  # draw bounding boxes\n",
    "\n",
    "    # Calculate distance for each detected block\n",
    "    for box in results[0].boxes.xywh:\n",
    "        w_pixels = box[2].item()\n",
    "        distance_inch = (KNOWN_WIDTH_INCH * FOCAL_LENGTH) / w_pixels\n",
    "        x, y = int(box[0]), int(box[1])\n",
    "        cv2.putText(annotated, f\"{distance_inch:.2f} in\", (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Show FPS on frame\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    cv2.putText(annotated, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # Display\n",
    "    cv2.imshow(\"YOLO Distance Estimation\", annotated)\n",
    "\n",
    "    # Stop with ESC\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
